{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb29943b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <font color = darkblue> This is an example of SGDRegressor and SGDRegressor\n",
    "    - Both use Nutrition data\n",
    "    - This demonstration shows how to tune hyperameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47bc422",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Importing the necessary libraries\n",
    "# ------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sklearn.metrics\n",
    "\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore') \n",
    "sns.set(rc={'figure.figsize':(11,8)})\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0911f1c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing nutrition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376d8cb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Volumes/LACIE SHARE/Courses/Roux /Machine Learning/Data'\n",
      "/Users/ZongyuWu/PycharmProjects/CS6140\n",
      "(6080, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   NDB_No               Shrt_Desc                   Long_Desc  \\\n0    1001        BUTTER,WITH SALT              Butter, salted   \n1    1002  BUTTER,WHIPPED,W/ SALT  Butter, whipped, with salt   \n2    1003    BUTTER OIL,ANHYDROUS       Butter oil, anhydrous   \n3    1004             CHEESE,BLUE                Cheese, blue   \n4    1005            CHEESE,BRICK               Cheese, brick   \n\n               FdGrp_Desc  Water_g  Energ_Kcal  Protein_g  Lipid_Tot_g  \\\n0  Dairy and Egg Products    15.87         717       0.85        81.11   \n1  Dairy and Egg Products    16.72         718       0.49        78.30   \n2  Dairy and Egg Products     0.24         876       0.28        99.48   \n3  Dairy and Egg Products    42.41         353      21.40        28.74   \n4  Dairy and Egg Products    41.11         371      23.24        29.68   \n\n   Carbohydrt_g  Fiber_TD_g  Sugar_Tot_g  Calcium_mg  Iron_mg  Magnesium_mg  \\\n0          0.06        0.00         0.06       24.00     0.02          2.00   \n1          2.87        0.00         0.06       23.00     0.05          1.00   \n2          0.00        0.00         0.00        4.00     0.00          0.00   \n3          2.34        0.00         0.50      528.00     0.31         23.00   \n4          2.79        0.00         0.51      674.00     0.43         24.00   \n\n   Phosphorus_mg  \n0          24.00  \n1          24.00  \n2           3.00  \n3         387.00  \n4         451.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NDB_No</th>\n      <th>Shrt_Desc</th>\n      <th>Long_Desc</th>\n      <th>FdGrp_Desc</th>\n      <th>Water_g</th>\n      <th>Energ_Kcal</th>\n      <th>Protein_g</th>\n      <th>Lipid_Tot_g</th>\n      <th>Carbohydrt_g</th>\n      <th>Fiber_TD_g</th>\n      <th>Sugar_Tot_g</th>\n      <th>Calcium_mg</th>\n      <th>Iron_mg</th>\n      <th>Magnesium_mg</th>\n      <th>Phosphorus_mg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1001</td>\n      <td>BUTTER,WITH SALT</td>\n      <td>Butter, salted</td>\n      <td>Dairy and Egg Products</td>\n      <td>15.87</td>\n      <td>717</td>\n      <td>0.85</td>\n      <td>81.11</td>\n      <td>0.06</td>\n      <td>0.00</td>\n      <td>0.06</td>\n      <td>24.00</td>\n      <td>0.02</td>\n      <td>2.00</td>\n      <td>24.00</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1002</td>\n      <td>BUTTER,WHIPPED,W/ SALT</td>\n      <td>Butter, whipped, with salt</td>\n      <td>Dairy and Egg Products</td>\n      <td>16.72</td>\n      <td>718</td>\n      <td>0.49</td>\n      <td>78.30</td>\n      <td>2.87</td>\n      <td>0.00</td>\n      <td>0.06</td>\n      <td>23.00</td>\n      <td>0.05</td>\n      <td>1.00</td>\n      <td>24.00</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1003</td>\n      <td>BUTTER OIL,ANHYDROUS</td>\n      <td>Butter oil, anhydrous</td>\n      <td>Dairy and Egg Products</td>\n      <td>0.24</td>\n      <td>876</td>\n      <td>0.28</td>\n      <td>99.48</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>4.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>3.00</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1004</td>\n      <td>CHEESE,BLUE</td>\n      <td>Cheese, blue</td>\n      <td>Dairy and Egg Products</td>\n      <td>42.41</td>\n      <td>353</td>\n      <td>21.40</td>\n      <td>28.74</td>\n      <td>2.34</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>528.00</td>\n      <td>0.31</td>\n      <td>23.00</td>\n      <td>387.00</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1005</td>\n      <td>CHEESE,BRICK</td>\n      <td>Cheese, brick</td>\n      <td>Dairy and Egg Products</td>\n      <td>41.11</td>\n      <td>371</td>\n      <td>23.24</td>\n      <td>29.68</td>\n      <td>2.79</td>\n      <td>0.00</td>\n      <td>0.51</td>\n      <td>674.00</td>\n      <td>0.43</td>\n      <td>24.00</td>\n      <td>451.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd \"/Volumes/LACIE SHARE/Courses/Roux /Machine Learning/Data\"\n",
    "nut = pd.read_csv(\"Final_Nutrition.csv\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Keeping only first 15 features for demonstration purposes, and no other reason\n",
    "# Also dropping  NAs\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "nut = nut.iloc[:,range(15)].dropna()\n",
    "\n",
    "print(nut.shape)\n",
    "\n",
    "nut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a9dd27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 213.25082828976738\n",
      "Number of adjustments in weights = 12001.0, Coefficients are [ 48.16033623 253.67507277  67.79826629], Number of iterations=  12 and the R2 score is 0.9963564017028033\n",
      "\n",
      "Note that the coefficients are for the standardized data and not on the original scale\n",
      "\n",
      "-----------\n",
      "30001.0 [ 48.22798122 253.59801307  67.74949403] 0.996 30\n",
      "-----------\n",
      "17001.0 [ 48.07889747 253.81126047  67.69073979] 0.996 17\n",
      "-----------\n",
      "15001.0 [ 48.15319313 253.65344147  67.83442312] 0.996 15\n",
      "-----------\n",
      "17001.0 [ 48.12792181 253.62094828  67.73166279] 0.996 17\n",
      "-----------\n",
      "22001.0 [ 48.20841048 253.59254339  67.63291751] 0.996 22\n",
      "-----------\n",
      "17001.0 [ 48.20667505 253.60423612  67.73384575] 0.996 17\n",
      "-----------\n",
      "20001.0 [ 48.14771603 253.62272575  67.71421772] 0.996 20\n",
      "-----------\n",
      "26001.0 [ 48.15700255 253.71738584  67.6580808 ] 0.996 26\n",
      "-----------\n",
      "24001.0 [ 48.17271412 253.77801213  67.65887794] 0.996 24\n",
      "-----------\n",
      "14001.0 [ 48.1329731  253.70880486  67.65045327] 0.996 14\n",
      "-----------\n",
      "19001.0 [ 48.06404865 253.76104073  67.65364737] 0.996 19\n",
      "-----------\n",
      "27001.0 [ 48.20752108 253.65855535  67.63000148] 0.996 27\n",
      "-----------\n",
      "25001.0 [ 48.2386465  253.76474324  67.721743  ] 0.996 25\n",
      "-----------\n",
      "20001.0 [ 48.21529091 253.69707271  67.74446005] 0.996 20\n",
      "-----------\n",
      "15001.0 [ 48.15404612 253.77936822  67.69895581] 0.996 15\n",
      "-----------\n",
      "19001.0 [ 48.23095833 253.66618747  67.89520287] 0.996 19\n",
      "-----------\n",
      "28001.0 [ 48.1069652  253.65684346  67.76091927] 0.996 28\n",
      "-----------\n",
      "18001.0 [ 48.0730484  253.73813336  67.82957267] 0.996 18\n",
      "-----------\n",
      "16001.0 [ 48.08957907 253.69966598  67.95331508] 0.996 16\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# # Separating the features and target\n",
    "# Notice no attention is paid to Test-Train separation. Consider that step as an \n",
    "# intergal part of ML pipeline, and should not be skipped\n",
    "# For simplicity, only a 1000 rows are used\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "X = nut.iloc[0:1000,[6,7,8]]\n",
    "y = nut['Energ_Kcal'][0:1000]\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # # # Feature Standardization\n",
    "# # This is a required step, as recommended by sklearn\n",
    "# # ----------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# # # Training the model\n",
    "# --------------------------\n",
    "model = SGDRegressor(max_iter=100, \n",
    "                     tol = 0.0001,\n",
    "                     early_stopping=False, warm_start=False,\n",
    "                     n_iter_no_change = 5)\n",
    "model.fit(X,y)\n",
    "\n",
    "    \n",
    "# -------\n",
    "# Predict    \n",
    "# -------\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# ----------------------\n",
    "# Evaluating using MSE\n",
    "# Using sklearn's built in function\n",
    "# ----------------------\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(f\"MSE is {mean_squared_error(y, y_pred)}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Alternatively writing the formula\n",
    "# --------------------------------------------\n",
    "\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of adjustments in weights = {model.t_}, Coefficients are {model.coef_}, Number of iterations=  {model.n_iter_} and the R2 score is {model.score(X,y)}\")\n",
    "print(\"\\nNote that the coefficients are for the standardized data and not on the original scale\\n\")\n",
    "    \n",
    "    \n",
    "# --------------------------------------------------------------------------------\n",
    "# Repeating the above step 20 times\n",
    "# The purpose is to illustrate how the results are similar but not exactly the same \n",
    "# This is due to the stochastic nature of the process\n",
    "# --------------------------------------------------------------------------------\n",
    "    \n",
    "for i in range(1,20):\n",
    "    # # # Training the model\n",
    "    model1 = SGDRegressor(max_iter=100, #tol=None, \n",
    "                         tol = 0.0001,\n",
    "                         early_stopping=False, warm_start=False,\n",
    "                         n_iter_no_change = 5)\n",
    "    model1.fit(X,y)\n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "    # # # Making predictions\n",
    "    y_pred = model1.predict(X)\n",
    "\n",
    "    # # # Evaluating the model\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    print(\"-----------\")\n",
    "    print(model1.t_,model1.coef_, \"{:.3f}\".format(model1.score(X,y)), model1.n_iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58478763",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-31e35e7000ad>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnut\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnut\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Energ_Kcal'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mmodel_comp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLinearRegression\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0my_pred_comp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_comp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Comparison with MLR\n",
    "# ---------------------\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Creating X and y again, and this time not standardizing them\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "X = nut.iloc[0:1000,[6,7,8]]\n",
    "y = nut['Energ_Kcal'][0:1000]\n",
    "model_comp = LinearRegression().fit(X, y)\n",
    "\n",
    "y_pred_comp = model_comp.predict(X)\n",
    "mse = np.mean((y - y_pred_comp)**2)\n",
    "print(model_comp.coef_, \"{:.3f}\".format(model_comp.score(X,y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2377f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <font  color = darkblue> Example of Grid Search\n",
    "    - Grid Search is used to find the most optimal setting of hyperparameters\n",
    "    - This is done by providing various values of hyperparameters to the model\n",
    "    - And then observing which setting provides the best results in terms of R-sq or MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac53da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "X = nut.iloc[0:1000,6:]\n",
    "y = nut['Energ_Kcal'][0:1000]\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # # # Feature Standardization\n",
    "# # This is a required step, as recommended by sklearn\n",
    "# # ----------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\n",
    "              'tol':[0.0001, 0.001, 0.1],\n",
    "             'max_iter':[50,100,1000],\n",
    "             'n_iter_no_change': [10,50,100]}\n",
    "\n",
    "# Create the SGDRegressor\n",
    "sgd_reg = SGDRegressor(alpha = 0.01,\n",
    "                       early_stopping=True,\n",
    "                       warm_start=True,\n",
    "                       l1_ratio=0.15,\n",
    "                       eta0 = 0.001)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "sgd_cv = GridSearchCV(sgd_reg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "sgd_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Parameters: {}\".format(sgd_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(sgd_cv.best_score_))\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Single run\n",
    "# -------------\n",
    "\n",
    "model4 = SGDRegressor(max_iter=1000, #tol=None, \n",
    "                      alpha = 0.01,\n",
    "                     tol = 0.1,\n",
    "                     l1_ratio=0.15,\n",
    "                     early_stopping=True, warm_start=True,\n",
    "                     eta0 = 0.001,\n",
    "                     n_iter_no_change = 100)\n",
    "model4.fit(X,y)\n",
    "model4.score(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ced268",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffca46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "#importing necessary libraries\n",
    "# ----------------------------------------\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "X = nut.iloc[0:1000,8:]\n",
    "y = nut['Energ_Kcal'][0:1000]\n",
    "\n",
    "# High when calories > 160, else Low\n",
    "y_cat = np.where(y<160, 0,1)\n",
    "\n",
    "# # # ----------------------------------------------------------------------\n",
    "# # # # # Feature Standardization\n",
    "# # # This is a required step, as recommended by sklearn\n",
    "# # # ----------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size = 0.2, random_state=42)\n",
    "\n",
    "#creating and fitting the SGDClassifier\n",
    "clf = SGDClassifier(max_iter=1000, tol=1e-3, loss = 'log')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#calculating the accuracy of the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fffdc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-e137fd54e0c8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# ------------------------------\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Overall accuracy is {sklearn.metrics.accuracy_score(y_pred, y_test)}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Precision or TP/(TP + FP) is {sklearn.metrics.precision_score(y_pred, y_test)}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Recall or TP / (TP + FN) is {sklearn.metrics.recall_score(y_pred, y_test)}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# help(sklearn.metrics)\n",
    "# ------------------------------\n",
    "# Various classification metrics\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"Overall accuracy is {sklearn.metrics.accuracy_score(y_pred, y_test)}\")\n",
    "print(f\"Precision or TP/(TP + FP) is {sklearn.metrics.precision_score(y_pred, y_test)}\")\n",
    "print(f\"Recall or TP / (TP + FN) is {sklearn.metrics.recall_score(y_pred, y_test)}\")\n",
    "print(f\"F1-score or 2*Precision*Recall / (Precision + Recall)  is {sklearn.metrics.f1_score(y_pred, y_test)}\")\n",
    "print(f\"Confusion Matrix \\n{sklearn.metrics.confusion_matrix(y_pred, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3b39c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Changing the decision threhold\n",
    "# -----------------------------------\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# # predict the decision scores for the data\n",
    "decision_scores = clf.decision_function(X_test)\n",
    "\n",
    "# # change the decision threshold to 0.6\n",
    "y_pred_new = (decision_scores > 0.6).astype(int)\n",
    "\n",
    "# # print the accuracy of the new predictions\n",
    "print(\"Accuracy with threshold of 0.6:\", sum(y_pred_new == y_test) / len(y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}